# -*- coding: utf-8 -*-
"""
å¤šAgentåä½œRAGç³»ç»Ÿ
å®ç°æ”¯æŒå¤šè·³æ¨ç†çš„Agentic RAGæ¶æ„
"""

from typing import List, Dict, Any, Optional
from agno.agent import Agent
from agno.models.deepseek import DeepSeek
from agno.tools.reasoning import ReasoningTools
from agno.tools.function import Function
from config.settings import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, DEFAULT_MODEL
from services.vector_store import VectorStoreService
import logging
import json

logger = logging.getLogger(__name__)

class PlannerAgent:
    """
    è§„åˆ’Agentï¼šè´Ÿè´£åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¶å®šå¤šè·³æ¨ç†è®¡åˆ’
    """
    
    def __init__(self):
        self.agent = Agent(
            name="Planner",
            model=DeepSeek(
                id=DEFAULT_MODEL,
                api_key=DEEPSEEK_API_KEY,
                base_url=DEEPSEEK_BASE_URL
            ),
            instructions="""
            ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢è§„åˆ’ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·é—®é¢˜å¹¶åˆ¶å®šæ£€ç´¢ç­–ç•¥ã€‚
            
            ã€æ ¸å¿ƒèŒè´£ã€‘
            1. åˆ†æç”¨æˆ·æŸ¥è¯¢çš„å¤æ‚åº¦å’Œä¿¡æ¯éœ€æ±‚
            2. åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šè·³æ¨ç†
            3. åˆ¶å®šæ£€ç´¢è®¡åˆ’å’Œæ¨ç†æ­¥éª¤
            4. è¯†åˆ«å…³é”®å®ä½“å’Œå…³ç³»
            
            ã€è¾“å‡ºæ ¼å¼ã€‘
            è¯·ä»¥JSONæ ¼å¼è¾“å‡ºè§„åˆ’ç»“æœï¼š
            {
                "query_type": "simple/multi_hop",
                "key_entities": ["å®ä½“1", "å®ä½“2"],
                "reasoning_steps": [
                    {
                        "step": 1,
                        "action": "search",
                        "target": "æœç´¢ç›®æ ‡",
                        "purpose": "æœç´¢ç›®çš„"
                    }
                ],
                "expected_hops": 2
            }
            """,
            tools=[ReasoningTools()],
            markdown=True
        )
    
    def plan_query(self, user_query: str) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶åˆ¶å®šæ£€ç´¢è®¡åˆ’
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            Dict: åŒ…å«è§„åˆ’ä¿¡æ¯çš„å­—å…¸
        """
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢å¹¶åˆ¶å®šæ£€ç´¢è®¡åˆ’ï¼š
        
        ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}
        
        è¯·ç‰¹åˆ«æ³¨æ„ï¼š
        1. å¦‚æœæŸ¥è¯¢æ¶‰åŠå¤šä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œæ ‡è®°ä¸ºmulti_hop
        2. è¯†åˆ«æ‰€æœ‰å…³é”®å®ä½“å’Œå¯èƒ½çš„ä¸­é—´å®ä½“
        3. åˆ¶å®šé€æ­¥çš„æ£€ç´¢å’Œæ¨ç†ç­–ç•¥
        """
        
        response = self.agent.run(prompt)
        
        try:
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            content = response.content
            if '```json' in content:
                json_start = content.find('```json') + 7
                json_end = content.find('```', json_start)
                json_str = content[json_start:json_end].strip()
            else:
                # å¯»æ‰¾JSONå¯¹è±¡
                start = content.find('{')
                end = content.rfind('}') + 1
                json_str = content[start:end]
            
            plan = json.loads(json_str)
            logger.info(f"æŸ¥è¯¢è§„åˆ’å®Œæˆ: {plan}")
            return plan
            
        except Exception as e:
            logger.error(f"è§£æè§„åˆ’ç»“æœå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è§„åˆ’
            return {
                "query_type": "simple",
                "key_entities": [user_query],
                "reasoning_steps": [
                    {
                        "step": 1,
                        "action": "search",
                        "target": user_query,
                        "purpose": "ç›´æ¥æœç´¢ç›¸å…³ä¿¡æ¯"
                    }
                ],
                "expected_hops": 1
            }

class RetrieverAgent:
    """
    æ£€ç´¢Agentï¼šè´Ÿè´£æ ¹æ®è§„åˆ’æ‰§è¡Œæ–‡æ¡£æ£€ç´¢
    """
    
    def __init__(self, vector_store: VectorStoreService):
        self.vector_store = vector_store
        self.agent = Agent(
            name="Retriever",
            model=DeepSeek(
                id=DEFAULT_MODEL,
                api_key=DEEPSEEK_API_KEY,
                base_url=DEEPSEEK_BASE_URL
            ),
            instructions="""
            ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æ£€ç´¢ä¸“å®¶ï¼Œè´Ÿè´£æ ¹æ®æ£€ç´¢è®¡åˆ’æ‰§è¡Œç²¾ç¡®çš„æ–‡æ¡£æœç´¢ã€‚
            
            ã€æ ¸å¿ƒèŒè´£ã€‘
            1. æ ¹æ®æ£€ç´¢è®¡åˆ’æ‰§è¡Œå¤šè½®æœç´¢
            2. ä¼˜åŒ–æœç´¢å…³é”®è¯
            3. è¯„ä¼°æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§
            4. è¯†åˆ«ä¿¡æ¯ç¼ºå£
            
            ã€æœç´¢ç­–ç•¥ã€‘
            1. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ç»„åˆ
            2. å…³æ³¨å®ä½“åç§°å’Œå…³ç³»è¯
            3. è€ƒè™‘åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µ
            """,
            tools=[ReasoningTools()],
            markdown=True
        )
    
    def retrieve_documents(self, plan: Dict[str, Any], similarity_threshold: float = 0.5) -> List[Dict[str, Any]]:
        """
        æ ¹æ®è§„åˆ’æ‰§è¡Œæ–‡æ¡£æ£€ç´¢
        
        Args:
            plan: æŸ¥è¯¢è§„åˆ’
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        all_documents = []
        
        for step in plan.get('reasoning_steps', []):
            if step['action'] == 'search':
                search_target = step['target']
                logger.info(f"æ‰§è¡Œæ£€ç´¢æ­¥éª¤ {step['step']}: {search_target}")
                
                # æ‰§è¡Œæœç´¢
                docs = self.vector_store.search_documents(search_target, similarity_threshold)
                
                # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ æ£€ç´¢æ­¥éª¤ä¿¡æ¯
                for doc in docs:
                    doc_info = {
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'retrieval_step': step['step'],
                        'search_target': search_target,
                        'purpose': step['purpose']
                    }
                    all_documents.append(doc_info)
        
        logger.info(f"æ€»å…±æ£€ç´¢åˆ° {len(all_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return all_documents
    
    def expand_search(self, entities: List[str], similarity_threshold: float = 0.5) -> List[Dict[str, Any]]:
        """
        åŸºäºå®ä½“è¿›è¡Œæ‰©å±•æœç´¢
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        expanded_docs = []
        
        for entity in entities:
            logger.info(f"æ‰©å±•æœç´¢å®ä½“: {entity}")
            docs = self.vector_store.search_documents(entity, similarity_threshold)
            
            for doc in docs:
                doc_info = {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'retrieval_step': 'expansion',
                    'search_target': entity,
                    'purpose': f'æ‰©å±•æœç´¢å®ä½“: {entity}'
                }
                expanded_docs.append(doc_info)
        
        return expanded_docs

class AnalyzerAgent:
    """
    åˆ†æAgentï¼šè´Ÿè´£åˆ†ææ£€ç´¢ç»“æœï¼Œæ‰§è¡Œå¤šè·³æ¨ç†
    """
    
    def __init__(self):
        self.agent = Agent(
            name="Analyzer",
            model=DeepSeek(
                id=DEFAULT_MODEL,
                api_key=DEEPSEEK_API_KEY,
                base_url=DEEPSEEK_BASE_URL
            ),
            instructions="""
            ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆ†æä¸“å®¶ï¼Œè´Ÿè´£åˆ†ææ£€ç´¢åˆ°çš„æ–‡æ¡£å¹¶æ‰§è¡Œå¤šè·³æ¨ç†ã€‚
            
            ã€æ ¸å¿ƒèŒè´£ã€‘
            1. åˆ†ææ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
            2. è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»
            3. æ‰§è¡Œå¤šè·³æ¨ç†ï¼Œè¿æ¥ä¿¡æ¯ç‰‡æ®µ
            4. åˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……åˆ†å›ç­”ç”¨æˆ·é—®é¢˜
            
            ã€æ¨ç†åŸåˆ™ã€‘
            1. åŸºäºäº‹å®è¿›è¡Œæ¨ç†ï¼Œä¸è¦è‡†æµ‹
            2. æ˜ç¡®æ ‡è¯†æ¨ç†é“¾æ¡
            3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®æŒ‡å‡ºç¼ºå¤±çš„ä¿¡æ¯
            4. æä¾›ç½®ä¿¡åº¦è¯„ä¼°
            
            ã€è¾“å‡ºæ ¼å¼ã€‘
            è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
            {
                "reasoning_chain": [
                    {
                        "step": 1,
                        "fact": "ä»æ–‡æ¡£ä¸­æå–çš„äº‹å®",
                        "source": "æ–‡æ¡£æ¥æº"
                    }
                ],
                "conclusion": "æœ€ç»ˆç»“è®º",
                "confidence": 0.9,
                "missing_info": ["ç¼ºå¤±çš„ä¿¡æ¯"],
                "need_more_search": false
            }
            """,
            tools=[ReasoningTools()],
            markdown=True
        )
    
    def analyze_documents(self, documents: List[Dict[str, Any]], user_query: str, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææ£€ç´¢åˆ°çš„æ–‡æ¡£å¹¶æ‰§è¡Œæ¨ç†
        
        Args:
            documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            user_query: ç”¨æˆ·æŸ¥è¯¢
            plan: æŸ¥è¯¢è§„åˆ’
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(documents):
            context_parts.append(f"""æ–‡æ¡£ {i+1} (æ£€ç´¢æ­¥éª¤: {doc['retrieval_step']}, ç›®æ ‡: {doc['search_target']}):
{doc['content']}
""")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜å¹¶æ‰§è¡Œå¤šè·³æ¨ç†ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š{user_query}
        
        æŸ¥è¯¢è§„åˆ’ï¼š{json.dumps(plan, ensure_ascii=False, indent=2)}
        
        æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
        {context}
        
        è¯·ç‰¹åˆ«æ³¨æ„ï¼š
        1. å¦‚æœè¿™æ˜¯ä¸€ä¸ªå¤šè·³é—®é¢˜ï¼Œè¯·æ˜ç¡®å±•ç¤ºæ¨ç†é“¾æ¡
        2. è¿æ¥ä¸åŒæ–‡æ¡£ä¸­çš„ä¿¡æ¯ç‰‡æ®µ
        3. è¯„ä¼°ä¿¡æ¯çš„å®Œæ•´æ€§å’Œå¯é æ€§
        4. å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
        """
        
        response = self.agent.run(prompt)
        
        try:
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            content = response.content
            if '```json' in content:
                json_start = content.find('```json') + 7
                json_end = content.find('```', json_start)
                json_str = content[json_start:json_end].strip()
            else:
                # å¯»æ‰¾JSONå¯¹è±¡
                start = content.find('{')
                end = content.rfind('}') + 1
                json_str = content[start:end]
            
            analysis = json.loads(json_str)
            logger.info(f"æ–‡æ¡£åˆ†æå®Œæˆ: {analysis}")
            return analysis
            
        except Exception as e:
            logger.error(f"è§£æåˆ†æç»“æœå¤±è´¥: {e}")
            # è¿”å›åŸå§‹å“åº”
            return {
                "reasoning_chain": [],
                "conclusion": response.content,
                "confidence": 0.5,
                "missing_info": [],
                "need_more_search": False
            }

class MultiAgentRAGSystem:
    """
    å¤šAgentåä½œRAGç³»ç»Ÿä¸»ç±»
    """
    
    def __init__(self, vector_store: VectorStoreService):
        self.planner = PlannerAgent()
        self.retriever = RetrieverAgent(vector_store)
        self.analyzer = AnalyzerAgent()
        self.vector_store = vector_store
        
    def process_query(self, user_query: str, similarity_threshold: float = 0.5, max_iterations: int = 3) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œæ‰§è¡Œå¤šAgentåä½œ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
        
        # ç¬¬ä¸€æ­¥ï¼šè§„åˆ’
        print("ğŸ¤” Planner Agent æ­£åœ¨åˆ†ææŸ¥è¯¢...")
        plan = self.planner.plan_query(user_query)
        print(f"ğŸ“‹ æŸ¥è¯¢è§„åˆ’å®Œæˆ: {plan['query_type']} ç±»å‹ï¼Œé¢„æœŸ {plan['expected_hops']} è·³æ¨ç†")
        
        iteration = 0
        all_documents = []
        final_analysis = None
        
        while iteration < max_iterations:
            iteration += 1
            print(f"\nğŸ” ç¬¬ {iteration} è½®æ£€ç´¢å’Œåˆ†æ...")
            
            # ç¬¬äºŒæ­¥ï¼šæ£€ç´¢
            print("ğŸ“š Retriever Agent æ­£åœ¨æ£€ç´¢æ–‡æ¡£...")
            if iteration == 1:
                # é¦–æ¬¡æ£€ç´¢ï¼šæŒ‰ç…§è§„åˆ’æ‰§è¡Œ
                documents = self.retriever.retrieve_documents(plan, similarity_threshold)
            else:
                # åç»­æ£€ç´¢ï¼šåŸºäºç¼ºå¤±ä¿¡æ¯æ‰©å±•æœç´¢
                if final_analysis and final_analysis.get('missing_info'):
                    documents = self.retriever.expand_search(final_analysis['missing_info'], similarity_threshold)
                else:
                    break
            
            if not documents:
                print("âŒ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
                break
                
            all_documents.extend(documents)
            print(f"âœ… æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ
            print("ğŸ§  Analyzer Agent æ­£åœ¨åˆ†ææ–‡æ¡£...")
            analysis = self.analyzer.analyze_documents(all_documents, user_query, plan)
            final_analysis = analysis
            
            print(f"ğŸ“Š åˆ†æå®Œæˆï¼Œç½®ä¿¡åº¦: {analysis.get('confidence', 0)}")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢
            if not analysis.get('need_more_search', False) or analysis.get('confidence', 0) > 0.8:
                print("âœ… åˆ†æå®Œæˆï¼Œä¿¡æ¯å……åˆ†")
                break
            
            print("ğŸ”„ éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå‡†å¤‡ä¸‹ä¸€è½®æ£€ç´¢...")
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            'user_query': user_query,
            'plan': plan,
            'total_documents': len(all_documents),
            'iterations': iteration,
            'analysis': final_analysis,
            'documents': all_documents
        }
        
        logger.info(f"æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œå…± {iteration} è½®è¿­ä»£")
        return result